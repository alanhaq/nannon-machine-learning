# nannon-machine-learning

#### This project is a continuation of nannon-strategy-tournament where machine learing in used to boost player strategy performance. Approaches that are used include: using a value hash table to find the best value of the possible legal moves, using a 3-layer neural network to estimate the strength of each legal move, and implementation of Expectimax algorithm (Expectimax finds all the legal moves for the given position then looks at all the possible counter moves the opponent can make. Based off these counter moves, it finds the strongest possible move the opponent can make for each legal move). 

#### Additionally, three machine learning algorithms were implemented including: 
#### The Pollack & Blaire Hillclimbing algorithm where two players using random neural networks compete over the course of 1000 games. Following each game iteration, the winning player's neural network would be preserved would replace the loser's along with randomly introduced noise that would affect the winning player's given weights. The strongest player at the conclusion of 1000 games would be the Hillclimbing player.
#### The Matchbox Reinforcement algorithm where two players using "matchboxes" are given a value of 25 beads per box and each box represents each possible move compete over the course of 1000 games. Following each game, each matchbox analogous to the positions played during the game of the winning player would be rewarded with more beads while the matchboxes of the loser would be penalized by taking away beads of the respective positions. Over the course of play, beads would shift towards more favorable positions leading to the Matchbox player.
#### The Neurogammon-style Backpropagation algorithm where a neural network is trained by using references to a mediocre value table. The algorithm compares pairs of moves using the value table and learning which of the two moves is better. For length of moves higher than 1, each subsequent move will be will be compared based on their value in the table and then the two are trained against one another where the higher value move is returned. Thus the Backpropagation algorithm returns the highest value move. 
